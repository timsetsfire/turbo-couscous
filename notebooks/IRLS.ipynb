{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab5dd9e-ce4a-4a09-8f1f-2f4cb93a2616",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn \n",
    "org.scalanlp breeze_2.11 0.13.2\n",
    "org.scalanlp breeze-natives_2.11 0.13.2\n",
    "org.scalanlp breeze-viz_2.11 0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ae7d38-9396-4515-b1d6-33906135bbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add jar jars/enets_2.11-1.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version 2.11.12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Properties.versionString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg._\r\n",
       "import breeze.numerics._\r\n",
       "import breeze.stats._\r\n",
       "import breeze.numerics._\r\n",
       "import breeze.stats.distributions.Gaussian\r\n",
       "import breeze.optimize.{LBFGSB, LBFGS, StochasticGradientDescent, ApproximateGradientFunction, DiffFunction}\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "import breeze.numerics._\n",
    "import breeze.stats._\n",
    "import breeze.numerics._\n",
    "import breeze.stats.distributions.Gaussian\n",
    "import breeze.optimize.{LBFGSB, LBFGS,StochasticGradientDescent, ApproximateGradientFunction, DiffFunction}\n",
    "\n",
    "//https://github.com/scalanlp/breeze/pull/633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined type alias RealVector\r\n",
       "defined type alias Features\r\n",
       "defined type alias Target\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type RealVector = DenseVector[Double]\n",
    "type Features = DenseMatrix[Double]\n",
    "type Target = DenseVector[Double]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x = breeze.linalg.csvread(new java.io.File(\"xout.csv\"))\n",
    "val y = breeze.linalg.csvread(new java.io.File(\"yout.csv\")).toDenseVector\n",
    "val xs = DenseMatrix.horzcat(x,DenseMatrix.ones[Double](x.rows,1))\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oct 18, 2018 12:21:06 PM com.github.fommil.netlib.LAPACK <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "Oct 18, 2018 12:21:06 PM com.github.fommil.jni.JniLoader liberalLoad\n",
      "INFO: successfully loaded C:\\Users\\WHITTA~1\\AppData\\Local\\Temp\\jniloader3283675447525051930netlib-native_ref-win-x86_64.dll\n",
      "Oct 18, 2018 12:21:06 PM com.github.fommil.netlib.BLAS <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "Oct 18, 2018 12:21:06 PM com.github.fommil.jni.JniLoader load\n",
      "INFO: already loaded netlib-native_ref-win-x86_64.dll\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.github.timsetsfire.enets.RlmNet\n",
    "import com.github.timsetsfire.enets.robust.norms.HuberT\n",
    "val rlm = new RlmNet(x,y, standardizeTarget =true, standardizeFeatures=true,rnorm=HuberT())\n",
    "rlm.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6b79bb-6e1e-4511-9ae1-a30f87b423ea",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val plot = new Plot\n",
    "for (i <- 0 until x.cols) {\n",
    "        plot.add(new Line { x = 1 to 100; y = rlm.b(i,::).t.toArray })\n",
    "}\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mad: (x: RealVector, c: Double, center: RealVector => Double)Double\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mad(x: RealVector,c: Double = Gaussian(0,1).inverseCdf(3/4d),center: (RealVector => Double) = (x: RealVector) => 0d) = {\n",
    "  val m = center(x)\n",
    "  median( abs( x - m) / c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class HuberRegressor\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HuberRegressor(epsilon: Double = 1.345, alpha: Double = 1d, maxIters:Int = 100, tolerance:Double = 1e-4) { \n",
    "    \n",
    "    private val epsSquare = math.pow(epsilon,2)\n",
    "    \n",
    "    def loss(x: Features, y: Target)(b: RealVector) = {\n",
    "        val sigma = b(-1)\n",
    "        val intercept = b(-2)\n",
    "        val nSamples = x.rows\n",
    "        val r = ( y - x*b(0 to -3)).map{ _ - b(-2)}\n",
    "        sum(r.map{ \n",
    "            elem => \n",
    "            if( abs(elem) < epsilon*sigma) {\n",
    "                math.pow(elem,2)/sigma\n",
    "            } else {\n",
    "                2*epsilon*abs(elem) - epsSquare*sigma\n",
    "            }}) + sigma*nSamples\n",
    "    }\n",
    "\n",
    "    def lossGrad(x: Features, y: Target)(b: RealVector) = { \n",
    "        val sigma = b(-1)\n",
    "        val intercept = b(-2)\n",
    "        val nSamples = x.rows.toDouble\n",
    "        val temp = DenseVector.zeros[Double](b.length)\n",
    "        val r = ( y - x*b(0 to -3)).map{ _ - b(-2)}\n",
    "        val psi = r.map{ elem => if( abs(elem) < epsilon*sigma) 2*elem/sigma else 2*epsilon*signum(elem)}\n",
    "        temp(0 to -3) := (-x.t * psi).map{ _ / nSamples.toDouble}\n",
    "        temp(-2) = -mean(psi) \n",
    "        temp(-1) = (x.rows - sum(r.map{ elem => if( abs(elem) < epsilon*sigma) math.pow(elem/sigma,2) else epsilon*epsilon }))/nSamples.toDouble\n",
    "        temp\n",
    "    }\n",
    "    \n",
    "    def maximumLikelihood(x: Features, y: Target, init: RealVector): RealVector = {\n",
    "        val diffFunction = new DiffFunction[RealVector] {\n",
    "            override def calculate(w: RealVector): (Double, RealVector) = (loss(x,y)(init), lossGrad(x,y)(init))\n",
    "        }\n",
    "        \n",
    "        val autograd = new ApproximateGradientFunction(loss(x,y))\n",
    "        \n",
    "        val lower: DenseVector[Double] = DenseVector.fill(init.length){-Inf}\n",
    "        lower(-1) = 1e-3\n",
    "        val upper: DenseVector[Double] = DenseVector.fill(init.length){Inf}\n",
    "             \n",
    "        val lbfgs = new LBFGSB(lower, upper, m=pow(init.length,2)/2, maxIter=maxIters, tolerance = tolerance)\n",
    "        lbfgs.minimize(autograd, init)\n",
    "    }\n",
    "       \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587106.1745260417\n",
      "DenseVector(-1.0320324412307829, 0.7987108549197264, 0.005406138728977784, 0.3973413985715523, -1.0596624608020682, 3.6063955670812926, -0.7507035695814653, -2.1883290546686305, 1.5977129032133814, -1.7669662468548923, -1.5937921776630397, 1.0428716125043005, -2.3052048326161074, 16.41302450887255, 1.9865378858301952)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.stats._\n",
    "// x(0, ::) \n",
    "val xs = x(*, ::) / stddev(x(::, *)).t\n",
    "val ys = y / stddev(y)\n",
    "OutputCell.HIDDEN\n",
    "\n",
    "stddev(xs(::, *))\n",
    "\n",
    "val hr = new HuberRegressor()\n",
    "// val q = inv(xs.t * xs) * xs.t * y\n",
    "val q = DenseVector.zeros[Double](x.cols + 2)\n",
    "q(-1) = 1\n",
    "\n",
    "q := hr.maximumLikelihood(xs,y,q)\n",
    "println(hr.loss(x,y)(q))\n",
    "println(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class HuberT\r\n",
       "mad: (x: RealVector, c: Double, center: RealVector => Double)Double\r\n",
       "defined class IRLS\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class HuberT(k: Double = 1.345) {\n",
    "  def rho(z: RealVector) = z.map{ elem => if( abs(elem) < k) pow(elem,2) else 2*k*abs(elem) - k*k }\n",
    "  def psi(z: RealVector) = z.map{ elem => if( abs(elem) < k) 2*elem else 2*k*signum(elem)}\n",
    "  def psiDeriv(z: RealVector) = z.map{ elem => if( abs(elem) < k) 1d else 0d}\n",
    "  def w(z: RealVector) = z.map{ elem => if( abs(elem) < k) 1 else k / abs (elem) }\n",
    "}\n",
    "def mad(x: RealVector,c: Double = Gaussian(0,1).inverseCdf(3/4d),center: (RealVector => Double) = (x: RealVector) => 0d) = {\n",
    "  val m = center(x)\n",
    "  median( abs( x - m) / c)\n",
    "}\n",
    "\n",
    "class IRLS(epsilon: Double = 1.345, alpha: Double = 1d, maxIters:Int=10) { \n",
    " \n",
    "    val norm = HuberT()\n",
    "    \n",
    "    def loss(x: Features, y: Target, weight: RealVector)(b: RealVector) = {\n",
    "        val nSamples = x.rows\n",
    "        val r = ( y - x*b(0 to -2)).map{ _ - b(-1)}\n",
    "        mean( weight *:* r *:* r)\n",
    "    }\n",
    "    def lossGrad(x: Features, y: Target, weight: RealVector)(b: RealVector) = { \n",
    "        val nSamples = x.rows\n",
    "        val r = ( y - x*b(0 to -2)).map{ _ - b(-1)}\n",
    "        val temp = DenseVector.zeros[Double](b.length)\n",
    "        temp(0 to -2) := (x.t * (weight *:* r)).map{ _ / -x.rows.toDouble }\n",
    "        temp(-1) = -1*mean(weight *:* r)\n",
    "        temp\n",
    "    }\n",
    "        \n",
    "    def maximumLikelihood(x: Features, y: Target, init: RealVector): RealVector = {\n",
    "        \n",
    "        val r = (y - x * init(0 to -2)).map{ _ - init(-1)}\n",
    "        val weight = sqrt(norm.w(r /mad(r)))\n",
    "        val xs = if(init.length == x.cols) x else DenseMatrix.horzcat(x, DenseMatrix.ones[Double](x.rows, 1))\n",
    "        val wexog =  xs(::, *) * weight\n",
    "             \n",
    "        val diffFunction = new DiffFunction[RealVector] {\n",
    "            override def calculate(w: RealVector): (Double, RealVector) = (loss(xs,y,weight)(w), lossGrad(xs,y,weight)(w))\n",
    "        }\n",
    "                   \n",
    "        for(i <- 0 to maxIters) { \n",
    "            val r = (y - xs * init(0 to -1))\n",
    "            weight := sqrt(norm.w(r / mad(r)))\n",
    "//             init := inv( (xs(::,*)*weight).t * xs ) * xs.t * (weight *:* y)\n",
    "            init := (xs(::, *) * weight) \\ (y *:* weight)\n",
    "        }\n",
    "        init\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x = DenseMatrix.rand(1000,10, Gaussian(0,1))\n",
    "val y = DenseVector.rand(1000, Gaussian(0,1))\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hr = new IRLS(maxIters=10)\n",
    "val q = DenseVector.zeros[Double](x.cols + 1)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector(-0.0022477518702361046, -0.018730997701338778, 0.03011015647674628, 0.02056878287202675, 0.03670674006897167, -0.020815522726142, -0.0039395040036241235, -0.014959433027974735, -0.00888533826768804, 0.022574464978395135, -0.08904871898126585)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr.maximumLikelihood(x,y,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import breeze.linalg.qr\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import breeze.linalg.qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.github.timsetsfire.enets.RlmNet@3bf68335"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = DenseMatrix.rand(1000,100, Gaussian(0,1))\n",
    "val y = DenseVector.rand(1000, Gaussian(0,1))\n",
    "OutputCell.HIDDEN\n",
    "val rlm = new RlmNet(x,y,rnorm = com.github.timsetsfire.enets.robust.norms.HuberT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlm.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b92624-60c5-4bb6-b99b-3e717f40bfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val plot = new Plot\n",
    "for (i <- 0 until x.cols) {\n",
    "        plot.add(new Line { x = 1 to 100; y = rlm.b(i,::).t.toArray })\n",
    "}\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(y - xb)T W (y - xb)  + lambda ||b||2\n",
    "\n",
    "-x(y-xb)W + 2*b\n",
    "-xTWy + xTWxb + 2b = 0\n",
    "-xTWy = (xTWx + 2lambda)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector(-0.7990535813415973, 1.136152252385928, -0.027852892818770937, 0.7248986942712289, -0.3323877394029593, 4.165218258260862, -0.20462349150181308, -2.0394100690094383, 1.4902572679630801, -1.5836110448673066, -0.8490719903222631, 1.360805664346451, -2.972855832986659)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = x * inv(diag(stddev(xs(::, *))))\n",
    "\n",
    "diag(stddev(x(::, *))) * inv(t.t * t) * t.t * y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val i = qr( x )\n",
    "i.r.cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val weights = DenseVector.ones[Double](y.length)\n",
    "val endog = y\n",
    "val exog = DenseMatrix.horzcat(x , DenseMatrix.ones[Double](x.rows, 1))\n",
    "\n",
    "val wHalf = sqrt(weights)\n",
    "val wendog = endog *:* weights\n",
    "val wexog = exog(::, *).map{ elem => elem*:* wHalf}\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00632  18.0   2.31   0.0  0.538   6.575  65.2   4.09    1.0  ... (14 total)\n",
       "0.02731  0.0    7.07   0.0  0.469   6.421  78.9   4.9671  2.0  ...\n",
       "0.02729  0.0    7.07   0.0  0.469   7.185  61.1   4.9671  2.0  ...\n",
       "0.03237  0.0    2.18   0.0  0.458   6.998  45.8   6.0622  3.0  ...\n",
       "0.06905  0.0    2.18   0.0  0.458   7.147  54.2   6.0622  3.0  ...\n",
       "0.02985  0.0    2.18   0.0  0.458   6.43   58.7   6.0622  3.0  ...\n",
       "0.08829  12.5   7.87   0.0  0.524   6.012  66.6   5.5605  5.0  ...\n",
       "0.14455  12.5   7.87   0.0  0.524   6.172  96.1   5.9505  5.0  ...\n",
       "0.21124  12.5   7.87   0.0  0.524   5.631  100.0  6.0821  5.0  ...\n",
       "0.17004  12.5   7.87   0.0  0.524   6.004  85.9   6.5921  5.0  ...\n",
       "0.22489  12.5   7.87   0.0  0.524   6.377  94.3   6.3467  5.0  ...\n",
       "0.11747  12.5   7.87   0.0  0.524   6.009  82.9   6.2267  5.0  ...\n",
       "0.09378  12.5   7.87   0.0  0.524   5.889  39.0   5.4509  5.0  ...\n",
       "0.62976  0.0    8.14   0.0  0.538   5.949  61.8   4.7075  4.0  ...\n",
       "0.63796  0.0    8.14   0.0  0.538   6.096  84.5   4.4619  4.0  ...\n",
       "0.62739  0.0    8.14   0.0  0.538   5.834  56.5   4.4986  4.0  ...\n",
       "1.05393  0.0    8.14   0.0  0.538   5.935  29.3   4.4986  4.0  ...\n",
       "0.7842   0.0    8.14   0.0  0.538   5.99   81.7   4.2579  4.0  ...\n",
       "0.80271  0.0    8.14   0.0  0.538   5.456  36.6   3.7965  4.0  ...\n",
       "0.7258   0.0    8.14   0.0  0.538   5.727  69.5   3.7965  4.0  ...\n",
       "1.25179  0.0    8.14   0.0  0.538   5.57   98.1   3.7979  4.0  ...\n",
       "0.85204  0.0    8.14   0.0  0.538   5.965  89.2   4.0123  4.0  ...\n",
       "1.23247  0.0    8.14   0.0  0.538   6.142  91.7   3.9769  4.0  ...\n",
       "0.98843  0.0    8.14   0.0  0.538   5.813  100.0  4.0952  4.0  ...\n",
       "0.75026  0.0    8.14   0.0  0.538   5.924  94.1   4.3996  4.0  ...\n",
       "0.84054  0.0    8.14   0.0  0.538   5.599  85.7   4.4546  4.0  ...\n",
       "0.67191  0.0    8.14   0.0  0.538   5.813  90.3   4.682   4.0  ...\n",
       "0.95577  0.0    8.14   0.0  0.538   6.047  88.8   4.4534  4.0  ...\n",
       "0.77299  0.0    8.14   0.0  0.538   6.495  94.4   4.4547  4.0  ...\n",
       "1.00245  0.0    8.14   0.0  0.538   6.674  87.3   4.239   4.0  ...\n",
       "1.13081  0.0    8.14   0.0  0.538   5.713  94.1   4.233   4.0  ...\n",
       "1.35472  0.0    8.14   0.0  0.538   6.072  100.0  4.175   4.0  ...\n",
       "1.38799  0.0    8.14   0.0  0.538   5.95   82.0   3.99    4.0  ...\n",
       "1.15172  0.0    8.14   0.0  0.538   5.701  95.0   3.7872  4.0  ...\n",
       "1.61282  0.0    8.14   0.0  0.538   6.096  96.9   3.7598  4.0  ...\n",
       "0.06417  0.0    5.96   0.0  0.499   5.933  68.2   3.3603  5.0  ...\n",
       "0.09744  0.0    5.96   0.0  0.499   5.841  61.4   3.3779  5.0  ...\n",
       "0.08014  0.0    5.96   0.0  0.499   5.85   41.5   3.9342  5.0  ...\n",
       "0.17505  0.0    5.96   0.0  0.499   5.966  30.2   3.8473  5.0  ...\n",
       "0.02763  75.0   2.95   0.0  0.428   6.595  21.8   5.4011  3.0  ...\n",
       "0.03359  75.0   2.95   0.0  0.428   7.024  15.8   5.4011  3.0  ...\n",
       "0.12744  0.0    6.91   0.0  0.448   6.77   2.9    5.7209  3.0  ...\n",
       "0.1415   0.0    6.91   0.0  0.448   6.169  6.6    5.7209  3.0  ...\n",
       "0.15936  0.0    6.91   0.0  0.448   6.211  6.5    5.7209  3.0  ...\n",
       "0.12269  0.0    6.91   0.0  0.448   6.069  40.0   5.7209  3.0  ...\n",
       "0.17142  0.0    6.91   0.0  0.448   5.682  33.8   5.1004  3.0  ...\n",
       "0.18836  0.0    6.91   0.0  0.448   5.786  33.3   5.1004  3.0  ...\n",
       "0.22927  0.0    6.91   0.0  0.448   6.03   85.5   5.6894  3.0  ...\n",
       "0.25387  0.0    6.91   0.0  0.448   5.399  95.3   5.87    3.0  ...\n",
       "0.21977  0.0    6.91   0.0  0.448   5.602  62.0   6.0877  3.0  ...\n",
       "0.08873  21.0   5.64   0.0  0.439   5.963  45.7   6.8147  4.0  ...\n",
       "0.04337  21.0   5.64   0.0  0.439   6.115  63.0   6.8147  4.0  ...\n",
       "0.0536   21.0   5.64   0.0  0.439   6.511  21.1   6.8147  4.0  ...\n",
       "0.04981  21.0   5.64   0.0  0.439   5.998  21.4   6.8147  4.0  ...\n",
       "0.0136   75.0   4.0    0.0  0.41    5.888  47.6   7.3197  3.0  ...\n",
       "0.01311  90.0   1.22   0.0  0.403   7.249  21.9   8.6966  5.0  ...\n",
       "0.02055  85.0   0.74   0.0  0.41    6.383  35.7   9.1876  2.0  ...\n",
       "0.01432  100.0  1.32   0.0  0.411   6.816  40.5   8.3248  5.0  ...\n",
       "0.15445  25.0   5.13   0.0  0.453   6.145  29.2   7.8148  8.0  ...\n",
       "0.10328  25.0   5.13   0.0  0.453   5.927  47.2   6.932   8.0  ...\n",
       "0.14932  25.0   5.13   0.0  0.453   5.741  66.2   7.2254  8.0  ...\n",
       "0.17171  25.0   5.13   0.0  0.453   5.966  93.4   6.8185  8.0  ...\n",
       "0.11027  25.0   5.13   0.0  0.453   6.456  67.8   7.2255  8.0  ...\n",
       "0.1265   25.0   5.13   0.0  0.453   6.762  43.4   7.9809  8.0  ...\n",
       "0.01951  17.5   1.38   0.0  0.4161  7.104  59.5   9.2229  3.0  ...\n",
       "0.03584  80.0   3.37   0.0  0.398   6.29   17.8   6.6115  4.0  ...\n",
       "0.04379  80.0   3.37   0.0  0.398   5.787  31.1   6.6115  4.0  ...\n",
       "0.05789  12.5   6.07   0.0  0.409   5.878  21.4   6.498   4.0  ...\n",
       "0.13554  12.5   6.07   0.0  0.409   5.594  36.8   6.498   4.0  ...\n",
       "0.12816  12.5   6.07   0.0  0.409   5.885  33.0   6.498   4.0  ...\n",
       "0.08826  0.0    10.81  0.0  0.413   6.417  6.6    5.2873  4.0  ...\n",
       "0.15876  0.0    10.81  0.0  0.413   5.961  17.5   5.2873  4.0  ...\n",
       "0.09164  0.0    10.81  0.0  0.413   6.065  7.8    5.2873  4.0  ...\n",
       "0.19539  0.0    10.81  0.0  0.413   6.245  6.2    5.2873  4.0  ...\n",
       "0.07896  0.0    12.83  0.0  0.437   6.273  6.0    4.2515  5.0  ...\n",
       "0.09512  0.0    12.83  0.0  0.437   6.286  45.0   4.5026  5.0  ...\n",
       "0.10153  0.0    12.83  0.0  0.437   6.279  74.5   4.0522  5.0  ...\n",
       "0.08707  0.0    12.83  0.0  0.437   6.14   45.8   4.0905  5.0  ...\n",
       "0.05646  0.0    12.83  0.0  0.437   6.232  53.7   5.0141  5.0  ...\n",
       "... (506 total)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog(::, *) * wHalf == exog(::, *).map{ elem => elem*:* wHalf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "input is incomplete",
     "evalue": "input is incomplete",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31minput is incomplete\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "// http://www.math.iit.edu/~fass/477577_Chapter_5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector(-0.09383025053527433, 0.05095982087221412, -0.016363368562327035, 2.145110781983357, -0.5019802061934725, 5.451399976856914, -0.006285366818846987, -0.9592717600484365, 0.18578045607241872, -0.010436638671028065, -0.4127724656574354, 0.014629513832620306, -0.45437374359116783, 2.9930858386819863)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val xtx = wexog.t * wexog\n",
    "val xty = wexog.t * wendog\n",
    "val c = cholesky(xtx + DenseMatrix.eye[Double](wexog.cols)*10d)\n",
    "val w = c \\ xty\n",
    "c.t \\ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseMatrix.eye()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QR(-3.013957672193257E-5   0.03086854015653992     ... (506 total)\n",
       "-1.3023921523367635E-4  -4.992343632689267E-7   ...\n",
       "-1.3014383682632836E-4  -4.988687576755804E-7   ...\n",
       "-1.5436995229271708E-4  -5.917325645276132E-7   ...\n",
       "-3.2929395136892535E-4  -1.262253122663938E-6   ...\n",
       "-1.423522729668707E-4   -5.45666266640384E-7    ...\n",
       "-4.210479792376889E-4   0.021434952484272768    ...\n",
       "-6.893474391075766E-4   0.021433924035892157    ...\n",
       "-0.0010073867384094396  0.02143270492422307     ...\n",
       "-8.10907219272586E-4    0.021433458071632973    ...\n",
       "-0.0010724825014244406  0.021432455398442848    ...\n",
       "-5.602050755584021E-4   0.021434419065791674    ...\n",
       "-4.472293520547114E-4   0.02143485212555237     ...\n",
       "-0.003003275290573417   -1.1512187205341663E-5  ...\n",
       "-0.0030423804375860914  -1.1662085476244543E-5  ...\n",
       "-0.0029919729493026806  -1.1468862948995335E-5  ...\n",
       "-0.0050261082428132015  -1.9266132274716938E-5  ...\n",
       "-0.0037397873521145732  -1.433539317585894E-5   ...\n",
       "-0.003828060068115135   -1.467376110200679E-5   ...\n",
       "-0.003461282402658451   -1.3267825002599365E-5  ...\n",
       "-0.005969686826706837   -2.2883067869941944E-5  ...\n",
       "-0.004063310909838945   -1.5575527163426206E-5  ...\n",
       "-0.005877551285208681   -2.2529892919473198E-5  ...\n",
       "-0.0047137439587485435  -1.8068774135187774E-5  ...\n",
       "-0.003577930194845039   -1.3714960576536537E-5  ...\n",
       "-0.004008468325613853   -1.53653039786234E-5    ...\n",
       "-0.0032042852840593     -1.2282700878336368E-5  ...\n",
       "-0.00455799101954928    -1.7471740290347736E-5  ...\n",
       "-0.003686327754796026   -1.4130471271368503E-5  ...\n",
       "-0.004780604222299483   -1.8325063617877837E-5  ...\n",
       "-0.005392742840658863   -2.0671519965816164E-5  ...\n",
       "-0.0064605518001232525  -2.476465677531196E-5   ...\n",
       "-0.006619213780746629   -2.5372841589085002E-5  ...\n",
       "-0.005492460965541184   -2.1053760556618542E-5  ...\n",
       "-0.007691410146949025   -2.9482796253365E-5     ...\n",
       "-3.0602161997601656E-4  -1.173045371199782E-6   ...\n",
       "-4.64683600599393E-4    -1.781230184972832E-6   ...\n",
       "-3.821812782433842E-4   -1.464981393921622E-6   ...\n",
       "-8.347995103132569E-4   -3.1999624782378336E-6  ...\n",
       "-1.3176526975124412E-4  0.12861889361549064     ...\n",
       "-1.601880351409443E-4   0.12861878466504006     ...\n",
       "-6.077512116213737E-4   -2.3296384931541247E-6  ...\n",
       "-6.748022319870084E-4   -2.586659186921755E-6   ...\n",
       "-7.599751497487609E-4   -2.9131449330590174E-6  ...\n",
       "-5.850988398762267E-4   -2.2428071776920835E-6  ...\n",
       "-8.174883293795972E-4   -3.1336050729478973E-6  ...\n",
       "-8.982738404033423E-4   -3.4432729643009333E-6  ...\n",
       "-0.0010933703726336498  -4.191119093890817E-6   ...\n",
       "-0.0012106858136716738  -4.640813906599476E-6   ...\n",
       "-0.0010480656291433558  -4.0174564629667415E-6  ...\n",
       "-4.2314630419934465E-4  0.03601180962689906     ...\n",
       "-2.0682807633410995E-4  0.03601263882026103     ...\n",
       "-2.556141316926053E-4   0.036012451813027944    ...\n",
       "-2.375399235001618E-4   0.03601252109527755     ...\n",
       "-6.485731699663121E-5   0.1286191500877761      ...\n",
       "-6.252054601660552E-5   0.1543430387850203      ...\n",
       "-9.800131355005671E-5   0.14576827619979035     ...\n",
       "-6.829093966115874E-5   0.17149226982582422     ...\n",
       "-7.365597507448301E-4   0.042870309511074206    ...\n",
       "-4.925340955450053E-4   0.04287124491284519     ...\n",
       "-7.120951892600715E-4   0.0428704032888949      ...\n",
       "-8.18871316286143E-4    0.04286999399349423     ...\n",
       "-5.258688489131267E-4   0.04287111713370939     ...\n",
       "-6.032684264760182E-4   0.042870820444814654    ...\n",
       "-9.304163636796138E-5   0.030010836381690158    ...\n",
       "-1.7091810596759282E-4  0.13719337011438645     ...\n",
       "-2.088310228884177E-4   0.1371932247861848      ...\n",
       "-2.760728000687486E-4   0.021435508204691728    ...\n",
       "-6.463794665973083E-4   0.02143408874118738     ...\n",
       "-6.111848342859011E-4   0.021434223649631195    ...\n",
       "-4.2090491162666694E-4  -1.6134172426693578E-6  ...\n",
       "-7.571137975283214E-4   -2.9021767668953923E-6  ...\n",
       "-4.3702386246847677E-4  -1.6752045787244494E-6  ...\n",
       "-9.317993505861598E-4   -3.5717833111847484E-6  ...\n",
       "-3.7655395220985297E-4  -1.4434106671331572E-6  ...\n",
       "-4.536197053470265E-4   -1.7388199424734786E-6  ...\n",
       "-4.8418848490205637E-4  -1.8559965176548832E-6  ...\n",
       "-4.152298963894617E-4   -1.5916637131115012E-6  ...\n",
       "-2.692532439433675E-4   -1.0321044359971915E-6  ...\n",
       "... (506 total),-209.69106694170537  -2.235205661528237  -154.89021853911055  ... (14 total)\n",
       "0.0                  583.1157722576634   35.25351136878955    ...\n",
       "0.0                  0.0                 -247.5718886244503   ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "0.0                  0.0                 0.0                  ...\n",
       "... (506 total))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val QR = qr(wexog)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 ms ± 17 ms per loop (mean ± std. dev. of 3 run, 10 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "QR.r \\ (QR.q.t * wendog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 ms ± 8 ms per loop (mean ± std. dev. of 3 run, 10 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "wexog \\ wendog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 ms ± 3 ms per loop (mean ± std. dev. of 3 run, 10 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "inv(wexog.t * wexog) * wexog.t * wendog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector(-0.10801135783679901, 0.04642045836688059, 0.02055862636706894, 2.686733819344869, -17.766611228299915, 3.809865206809225, 6.92224640344334E-4, -1.4755668456002504, 0.3060494789851739, -0.012334593916574477, -0.9527472317072868, 0.00931168327379386, -0.5247583778554888, 36.45948838508974)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exog \\ endog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 ms ± 5 ms per loop (mean ± std. dev. of 3 run, 10 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "LSMR.solve(exog, endog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 ms ± 54 ms per loop (mean ± std. dev. of 3 run, 10 loop each)\n"
     ]
    }
   ],
   "source": [
    "self.endog = endog\n",
    "self.e`xog = exog\n",
    "self.weights = weights\n",
    "w_half = np.sqrt(weights)\n",
    "\n",
    "self.wendog = w_half * endog\n",
    "if np.isscalar(weights):\n",
    "    self.wexog = w_half * exog\n",
    "else:\n",
    "    self.wexog = w_half[:, None] * exog\n",
    "Q, R = np.linalg.qr(self.wexog)\n",
    "params = np.linalg.solve(R, np.dot(Q.T, self.wendog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MinimalWLS(object):\n",
    "    \"\"\"\n",
    "    Minimal implementation of WLS optimized for performance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    endog : array-like\n",
    "        1-d endogenous response variable. The dependent variable.\n",
    "    exog : array-like\n",
    "        A nobs x k array where `nobs` is the number of observations and `k`\n",
    "        is the number of regressors. An intercept is not included by default\n",
    "        and should be added by the user. See\n",
    "        :func:`statsmodels.tools.add_constant`.\n",
    "    weights : array-like, optional\n",
    "        1d array of weights.  If you supply 1/W then the variables are pre-\n",
    "        multiplied by 1/sqrt(W).  If no weights are supplied the default value\n",
    "        is 1 and WLS reults are the same as OLS.\n",
    "    Notes\n",
    "    -----\n",
    "    Need resid, scale, fittedvalues, model.weights!\n",
    "        history['scale'].append(tmp_results.scale)\n",
    "        if conv == 'dev':\n",
    "            history['deviance'].append(self.deviance(tmp_results))\n",
    "        elif conv == 'sresid':\n",
    "            history['sresid'].append(tmp_results.resid/tmp_results.scale)\n",
    "        elif conv == 'weights':\n",
    "            history['weights'].append(tmp_results.model.weights)\n",
    "    Does not perform and checks on the input data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endog, exog, weights=1.0):\n",
    "        self.endog = endog\n",
    "        self.exog = exog\n",
    "        self.weights = weights\n",
    "        w_half = np.sqrt(weights)\n",
    "\n",
    "        self.wendog = w_half * endog\n",
    "        if np.isscalar(weights):\n",
    "            self.wexog = w_half * exog\n",
    "        else:\n",
    "            self.wexog = w_half[:, None] * exog\n",
    "\n",
    "    def fit(self, method='pinv'):\n",
    "        \"\"\"\n",
    "        Minimal implementation of WLS optimized for performance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, optional\n",
    "            Method to use to estimate parameters.  \"pinv\", \"qr\" or \"lstsq\"\n",
    "              * \"pinv\" uses the Moore-Penrose pseudoinverse\n",
    "                 to solve the least squares problem.\n",
    "              * \"qr\" uses the QR factorization.\n",
    "              * \"lstsq\" uses the least squares implementation in numpy.linalg\n",
    "        Returns\n",
    "        -------\n",
    "        results : namedtuple\n",
    "            Named tuple containing the fewest terms needed to implement\n",
    "            iterative estimation in models. Currently\n",
    "              * params : Estimated parameters\n",
    "              * fittedvalues : Fit values using original data\n",
    "              * resid : Residuals using original data\n",
    "              * model : namedtuple with one field, weights\n",
    "              * scale : scale computed using weighted residuals\n",
    "        Notes\n",
    "        -----\n",
    "        Does not perform and checks on the input data\n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.regression.linear_model.WLS\n",
    "        \"\"\"\n",
    "        if method == 'pinv':\n",
    "            pinv_wexog = np.linalg.pinv(self.wexog)\n",
    "            params = pinv_wexog.dot(self.wendog)\n",
    "        elif method == 'qr':\n",
    "            Q, R = np.linalg.qr(self.wexog)\n",
    "            params = np.linalg.solve(R, np.dot(Q.T, self.wendog))\n",
    "        else:\n",
    "            params, _, _, _ = np.linalg.lstsq(self.wexog, self.wendog,\n",
    "                                              rcond=-1)\n",
    "\n",
    "        fitted_values = self.exog.dot(params)\n",
    "        resid = self.endog - fitted_values\n",
    "        wresid = self.wendog - self.wexog.dot(params)\n",
    "        df_resid = self.wexog.shape[0] - self.wexog.shape[1]\n",
    "        scale = np.dot(wresid, wresid) / df_resid\n",
    "\n",
    "        return Bunch(params=params, fittedvalues=fitted_values, resid=resid,\n",
    "                     model=self, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import breeze.stats._\n",
    "val xs = DenseMatrix.horzcat(x, DenseMatrix.ones[Double](x.rows, 1))\n",
    "\n",
    "val r = (y - xs * q)\n",
    "val weight = HuberT().w(r / mad(r))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for(i <- 0 to 10) { \n",
    "    r := (y - xs *q )\n",
    "    weight := HuberT().w(r / mad(r))\n",
    "    q := inv( (xs(::,*)*weight).t * xs ) * xs.t * (weight *:* y)\n",
    "    println(q(-1))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ag = new ApproximateGradientFunction(hr.loss(xs,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.calculate(q)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr.lossGrad(x,y)(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr.lossGrad(x,y)(qSklearn) - ag.calculate(qSklearn)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val qSklearn = DenseVector(-0.11828755,  0.02990455, -0.02083657,  1.83264527, -0.33127537,  6.16875147,\n",
    " -0.038073,   -0.8278615 ,  0.13821192, -0.01013945, -0.56633751,  0.01356108,\n",
    " -0.27942803, 1.0998216367119165, 1.9464638350773724)\n",
    "hr.loss(x,y)(qSklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val norm = HuberT()\n",
    "\n",
    "def loss(x: Features, y: Target, weight: RealVector)(b: RealVector) = {\n",
    "    val nSamples = x.rows\n",
    "    val r = ( y - x*b(0 to -2)).map{ _ - b(-1)}\n",
    "    mean( weight *:* r *:* r)\n",
    "}\n",
    "def lossGrad(x: Features, y: Target, weight: RealVector)(b: RealVector) = { \n",
    "    val nSamples = x.rows\n",
    "    val r = ( y - x*b(0 to -2)).map{ _ - b(-1)}\n",
    "    val temp = DenseVector.zeros[Double](b.length)\n",
    "    temp(0 to -2) := (x.t * (weight *:* r)).map{ _ / -x.rows.toDouble }\n",
    "    temp(-1) = -1*mean(weight *:* r)\n",
    "    temp\n",
    "}\n",
    "val weight = DenseVector.ones[Double](y.length)\n",
    "var c = mad(y)\n",
    "\n",
    "val diffFunction = new DiffFunction[RealVector] {\n",
    "    override def calculate(w: RealVector): (Double, RealVector) = (loss(x,y,weight)(w), lossGrad(x,y,weight)(w))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffFunction.calculate(b)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hr = new HuberRegressor()\n",
    "val q = DenseVector.zeros[Double](x.cols + 1)\n",
    "q(-1) = 1.0\n",
    "hr.loss(x,y)(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q := hr.maximumLikelihood(x,y,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr.loss(x,y)(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val b = q.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val epsilon = 1.345\n",
    "val sigma = b(-1)\n",
    "val intercept = b(-2)\n",
    "val nSamples = x.rows\n",
    "val r = ( y - x*b(0 to -3)).map{ _ - b(-2)}\n",
    "println(\"square\", sum(r.map{ elem => if( abs(elem) < epsilon*sigma) math.pow(elem,2)/sigma else 0}) )\n",
    "println(\"outlier\", sum(r.map{ elem => if( abs(elem) < epsilon*sigma) 0 else 2*epsilon*abs(elem/sigma)*sigma - epsilon*epsilon*sigma}) )\n",
    "println(x.rows)\n",
    "println(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hr.loss(x,y)(q), hr.lossGrad(x,y)(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val autograd = new ApproximateGradientFunction(hr.loss(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd.calculate(q)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val b = q.copy     \n",
    "val sigma = b(-1)\n",
    "val intercept = b(-2)\n",
    "val nSamples = x.rows\n",
    "val r = ( y - x*b(0 to -3)).map{ _ - b(-2)}) \n",
    "\n",
    "println((sigma, intercept, nSamples))\n",
    "\n",
    "sum(r.map{ elem => if( abs(elem) < sigma*1.345) math.pow(elem/sigma,2) else 0d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr.lossGrad(x,y)(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val w = q.copy\n",
    "//costGrad(x,y)(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sigma = w(-1)\n",
    "val intercept = w(-2)\n",
    "val nSamples = x.rows\n",
    "val epsilon = 1.3456\n",
    "\n",
    "val k = 1.345\n",
    "val temp = DenseVector.zeros[Double](w.length)\n",
    "\n",
    "val r = ( y - x*w(0 to -3).map{ _ + w(-2)}) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val psi = r.map{ elem => if( abs(elem) < k) 2*elem else 2*k*signum(elem)}\n",
    "temp(0 to -3) := -x.t * psi\n",
    "temp(-2) = -sum(psi)\n",
    "temp(-1) = x.rows - sum(r.map{ elem => if( abs(elem) < k*sigma) math.pow(elem/sigma,2) else k*k })\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r.map{ elem => if( abs(elem) < k*sigma) math.pow(elem, 2)/sigma else (2*k*abs(elem/sigma) - k*k)*sigma}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r.map{ elem => if( abs(elem) < k*sigma) elem*elem/sigma else 2*k*abs(elem/sigma)*sigma - k*k*sigma}) + 506*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r.map{ elem => if( abs(elem) < k*sigma) 0 else 2*k*abs(elem/sigma)*sigma - k*k*sigma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(r.map{ elem => if( abs(elem) < k*sigma) 0d else k*k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples - sum(r.map{ elem => if( abs(elem) < k*sigma) 0d else k*k}) + sum(r.map{ elem => if( abs(elem) < k*sigma) elem*elem else 0d})/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val g = new ApproximateGradientFunction(cost(xs,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val upper = DenseVector.fill(x.cols + 2){Inf}\n",
    "val lower = DenseVector.fill(x.cols + 2){-Inf}\n",
    "lower(-1) = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lbfgsb = new LBFGSB(lower, upper, maxIter=1000)\n",
    "lbfgsb.minimize(g, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class HuberT(k: Double = 1.345) {\n",
    "  def rho(z: RealVector) = z.map{ elem => if( abs(elem) < k) pow(elem,2) else 2*k*abs(elem) - k*k }\n",
    "  def psi(z: RealVector) = z.map{ elem => if( abs(elem) < k) 2*elem else 2*k*signum(elem)}\n",
    "  def psiDeriv(z: RealVector) = z.map{ elem => if( abs(elem) < k) 1d else 0d}\n",
    "  def w(z: RealVector) = z.map{ elem => if( abs(elem) < k) 1 else k / abs (elem) }\n",
    "}\n",
    "def mad(x: RealVector,c: Double = Gaussian(0,1).inverseCdf(3/4d),center: (RealVector => Double) = (x: RealVector) => 0d) = {\n",
    "  val m = center(x)\n",
    "  median( abs( x - m) / c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val huber = HuberT()\n",
    "\n",
    "-xs.t * huber.psi( y - xs*w(0 to -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val r = y - xs*w(0 to -2)\n",
    "sum(huber.rho( r )) + 1.0*xs.rows\n",
    "\n",
    "-1*sum(huber.psi(r))*2 + xs.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val linearLoss = y - xs*init\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val absLinearLoss = abs(linearLoss)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierMask.activeSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val w = DenseVector.zeros[Double](x.cols + 2)\n",
    "w(-1) = 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val b = DenseVector.zeros[Double](xs.cols + 1)\n",
    "// b(-1) = 1d\n",
    "// val x = xs.copy\n",
    "\n",
    "\n",
    "// val sigma = b(-1)\n",
    "// val intercept = b(-2)\n",
    "// val nSamples = x.rows\n",
    "// //\n",
    "// val epsilon = 1.3456\n",
    "// val sampleWeight = DenseVector.ones[Double](y.length)\n",
    "// val linearLoss = y - x*b(0 to -2)\n",
    "// //\n",
    "// val nFeatures = x.cols\n",
    "\n",
    "// val outlierMask = BitVector(absLinearLoss.map{_ > 1.345*sigma}.toArray:_*)\n",
    "// val outliers = absLinearLoss(outlierMask)\n",
    "\n",
    "// val numOutliers = outlierMask.activeSize\n",
    "// val outliersSw = sampleWeight(outlierMask)\n",
    "// val nSwOutliers = sum(outliersSw)\n",
    "// val outlierLoss = (2.0 * epsilon * sum(outliersSw * outliers) - sigma * nSwOutliers * scala.math.pow(epsilon, 2))\n",
    "\n",
    "\n",
    "// val nonOutliers = linearLoss(!outlierMask)\n",
    "// val weightedNonOutliers = sampleWeight(!outlierMask) *:* nonOutliers\n",
    "// val weightedLoss = weightedNonOutliers.t * nonOutliers\n",
    "// val squaredLoss = weightedLoss / sigma\n",
    "\n",
    "// nSamples * sigma + squaredLoss + outlierLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(x,y)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import breeze.optimize.{LBFGS, ApproximateGradientFunction, DiffFunction}\n",
    "val g = new ApproximateGradientFunction(cost(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sigma = w(-1)\n",
    "val intercept = w(-2)\n",
    "val nFeatures = x.cols\n",
    "val epsilon = 1.345\n",
    "val sampleWeight = DenseVector.ones[Double](y.length)\n",
    "\n",
    "\n",
    "val linearLoss = y - xs*w(0 to -2)\n",
    "val absLinearLoss = abs(linearLoss)\n",
    "val outlierMask = BitVector(absLinearLoss.map{_ > 1.345*sigma}.toArray:_*)\n",
    "val outliers = absLinearLoss(outlierMask)\n",
    "\n",
    "val numOutliers = outlierMask.activeSize\n",
    "val outliersSw = sampleWeight(outlierMask)\n",
    "val nSwOutliers = sum(outliersSw)\n",
    "val outlierLoss = (2.0 * epsilon * sum(outliersSw * outliers) - sigma * nSwOutliers * scala.math.pow(epsilon, 2))\n",
    "\n",
    "val nonOutliers = linearLoss(!outlierMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(outliersSw * outliers) - sigma * nSwOutliers * scala.math.pow(epsilon, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(sum(outliersSw * outliers))\n",
    "\n",
    "println(sigma)\n",
    "println(nSwOutliers)\n",
    "println(scala.math.pow(epsilon, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z: RealVector, c: Double, k: Double = 1.345) = z.map{ elem => if( abs(elem) < 1.345*c) pow(elem,2) else 2*k*abs(elem) - k*k }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(g(r, 1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(HuberT().rho(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-xs.t * HuberT().psi(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(HuberT().psi(r/200d) *:* (r/25d)) * -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val weight = DenseVector.ones[Double](xs.rows)\n",
    "val resid = y.copy\n",
    "def g(xs: Features, y: Target, weight: RealVector)(b: RealVector) = { \n",
    "    val r = y - xs * b    \n",
    "    sum(weight *:* r *:* r ) / r.length.toDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xs.t * HuberT(1.35).psi(r)).map{ _ * -1d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuberT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cost = g(xs, y, weight)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diffg = new ApproximateGradientFunction(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lbfgs = new LBFGS[DenseVector[Double]](m=pow(init.length,2)/2,tolerance = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// val diffFunction = new DiffFunction[RealVector] {\n",
    "//   override def calculate(w: RealVector): (Double, RealVector) =\n",
    "//     (loss(w, x, y), lossGrad(w, x, y))\n",
    "// }\n",
    "for(i <- 0 to 10) {\n",
    "    init := lbfgs.minimize(diffg, init)\n",
    "    resid := y - xs * init\n",
    "    weight := HuberT().rho( resid / mad(resid))\n",
    "    println(init(0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(xs.t * xs) * xs.t * y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
